\documentclass{article}

\usepackage{amsmath}
\usepackage{minted}
\usepackage{graphicx}

\author{Austin Minor}
\title{Simulation Results of Channel Capacity using Parity Bits for Error Correction}
\date{\today}

\begin{document}
   \maketitle

   \section{Introduction}
      Claude Shannon was a major figure in creating the
      mathematical discipline of information theory. This
      paper analyizes how emperical results line up with
      the statements made by this mathematical model of
      information. More formally, we will be analyizing
      the effect of using parity bits for error correction
      and how well they are at approaching the formal limit
      on the conveyance of data.
   \section{Mathematical Description}
      The official description of converyable information
      is the mutual information between to system of events.
      Let $A$ be the source alphabet. Let $B$ be the reception
      alphabet. Let $Q$ be the transmission matrix (IE the
      probability of receiving $b_j \in B$ given a transmission
      of $a_i \in A$. Let these entries be represented as $q_{i,j}$
      respectively. For our example we will only consider the
      simplier channel known as the binary symettric channel.
      Thus \[ Q =
      \begin{pmatrix}
         q_{0,0} & q_{0,1} \\
         q_{0,1} & q_{1,1}
      \end{pmatrix}
      =
      \begin{pmatrix}
         p & 1-p \\
         1-p & p
      \end{pmatrix}
      \]
      Given such a channel, the probability of bit errors
      can be represented by a Bernoulli Trial.

      The mutual information between two system of events (definition
      given elsewhere) is defined as following:
      \[
         I(A,B) =
         \sum_{i=1}^{m}
         \sum_{j=1}^{n}
         Pr(A_i \cup B_j)
         \log \left(\frac{Pr(A_i \cup B_j)}{Pr(A_i)Pr(B_j)}\right)
      \]
      \[
         ... =
         \sum_{i=1}^{m}
         Pr(A_i)
         \sum_{j=1}^{n}
         q_{i,j}
         \log \left(\frac{q_{i,j}}{\sum_{t=1}^{m} Pr(A_t) q_{t,j}}\right)
      \]
      It has been proved elsewhere that the maximal channel capacity
      (maximum I(A,B) based on $Pr(A_i)$ -- probability that a source letter
      is transmitted) for a BSC is when $Pr(A_1=0)=Pr(A_2=1)=\frac{1}{2}$.
      Furthermore for these values, 
      \[
      I(A,B) = p*log(2*p) + (1-p)*log(2*(1-p))
      \]
      For example, if $p = .99$ then $I(A,B) = .919$.
   \section{Problem Statement}
      To test this model of channel capacity and mutual information,
      a Matlab program was written that would satisfy the important
      requirements listed above ($Pr(A_i) = \frac{1}{2}$, Q as described above).
      The necessary error correction was implemented using parity bits.
      The Matlab code is attached below.

      \begin{listing}[H]
         \inputminted[linenos]{matlab}{../main.m}
         \caption{Main Simulator}
      \end{listing}
      
      \begin{listing}[H]
         \inputminted[linenos]{matlab}{../transmit.m}
         \caption{Transmission Simulator}
      \end{listing}

      \begin{listing}[H]
         \inputminted[linenos]{matlab}{../transmit_msg.m}
         \caption{Per-packet Transmission Simulator}
      \end{listing}

      \begin{listing}[H]
         \inputminted[linenos]{matlab}{../insert_parity_bit.m}
         \caption{Per-packet Parity Bit Inserter}
      \end{listing}

      \begin{listing}[H]
         \inputminted[linenos]{matlab}{../parity.m}
         \caption{Parity Bit Message Inserter}
      \end{listing}

      \begin{listing}[H]
         \inputminted[linenos]{matlab}{../generate_random_msg.m}
         \caption{Generator of Random Messages with Packets}
      \end{listing}
   \section{Analysis}
      \includegraphics[width=\textwidth]{images/polyline288.png}
      Because the error correction was implemented using parity bits,
      the maximum channel capacity possible is $\frac{4}{5}$. This is since
      one new bit is added to each four bit transmission package thus lengthening
      the transmitted message at a $\frac{4}{5}$ rate.

      The average number of of bits needed follows a roughly linear pattern. This
      is of course till around 30 when the number of errors invalidates the
      scheme. It is of notice that around 25 the number of bits needed goes to
      0. From analysising the second graph on number of errant messages, we
      theorize that this tail end is due to the few times that message transmits
      successfully. Since this rarely happens after 20 according to the second
      graph, the tail end of the graph is less smooth and sporodic as the
      random events are smoothed over less and less successful counts.

      Since the goal of an error correction scheme is to reduce the number of
      errors to essentially zero, the second graph reveals that after 1\% error
      rate the number of errors is accountable and is thus too much. Before 1\%
      the number of errors are not accountable and the message is transmitted
      successfully.

      Analysizing the first and third graph, shows the channel is relatively
      ineffcient. Since the parity bit adds 1 bit of information for every 4 bits
      in our case, the channel capacity is low because we do not need this much
      information to detect and find errors. I.E. the max information at perfect
      transmission is 80\%. The channel capacity after 16 is less than 50\%.
      However, even before this as mentioned in the second graph, the encoding
      schemes fails.
   \section{Conclusion}
      Parity bits are an easy to implement and fast method for detecting bits
      in a byte packet. However, they are not very effecient as noted. Not only
      that, they are not applicable in high noise environments since outside of
      1\% errors the parity scheme starts to fail to transmit properly. However
      in low noise enviroments such as CPU to RAM, parity bits are a good scheme
      for quick, easly to implement error correction.
      
      Thanks should go to Pete Johnson, my teacher for Information Theory. Also
      to https://en.wikipedia.org/wiki/Parity\_bit for a description of parity
      bits and how they are calculated.
\end{document}
